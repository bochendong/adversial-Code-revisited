{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'GraphKeys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mK\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcleverhans\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils_tf\u001b[39;00m \u001b[39mimport\u001b[39;00m batch_eval, model_argmax\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcleverhans\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattacks\u001b[39;00m \u001b[39mimport\u001b[39;00m SaliencyMapMethod\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/cleverhans/utils_tf.py:368\u001b[0m\n\u001b[1;32m    363\u001b[0m         x_norm \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmultiply(x, x_inv_norm)\n\u001b[1;32m    364\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mreshape(x_norm, x_shape, name_scope)\n\u001b[1;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkl_with_logits\u001b[39m(\n\u001b[0;32m--> 368\u001b[0m     p_logits, q_logits, scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, loss_collection\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mGraphKeys\u001b[39m.\u001b[39mREGULARIZATION_LOSSES\n\u001b[1;32m    369\u001b[0m ):\n\u001b[1;32m    370\u001b[0m     \u001b[39m\"\"\"Helper function to compute kl-divergence KL(p || q)\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(scope, \u001b[39m\"\u001b[39m\u001b[39mkl_divergence\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m name:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'GraphKeys'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from cleverhans.utils_tf import batch_eval, model_argmax\n",
    "from cleverhans.attacks import SaliencyMapMethod\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from cleverhans.utils import other_classes\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset='mnist'):\n",
    "     # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # reshape to (n_samples, 28, 28, 1)\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    # one-hot-encode the labels\n",
    "    Y_train = np_utils.to_categorical(y_train, 10)\n",
    "    Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dataset='mnist'):\n",
    "    layers = [\n",
    "            Conv2D(64, (3, 3), padding='valid', input_shape=(28, 28, 1)),\n",
    "            Activation('relu'),\n",
    "            Conv2D(64, (3, 3)),\n",
    "            Activation('relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Dropout(0.5),\n",
    "            Flatten(),\n",
    "            Dense(128),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(10),\n",
    "            Activation('softmax')\n",
    "    ]\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 22:40:19.502672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 227s 482ms/step - loss: 2.2906 - accuracy: 0.1258 - val_loss: 2.2664 - val_accuracy: 0.2226\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 258s 550ms/step - loss: 2.2582 - accuracy: 0.1846 - val_loss: 2.2232 - val_accuracy: 0.4124\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 273s 581ms/step - loss: 2.2152 - accuracy: 0.2512 - val_loss: 2.1613 - val_accuracy: 0.5818\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 246s 525ms/step - loss: 2.1539 - accuracy: 0.3233 - val_loss: 2.0739 - val_accuracy: 0.6643\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 216s 460ms/step - loss: 2.0678 - accuracy: 0.3943 - val_loss: 1.9542 - val_accuracy: 0.6999\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 214s 457ms/step - loss: 1.9535 - accuracy: 0.4503 - val_loss: 1.7981 - val_accuracy: 0.7237\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 206s 440ms/step - loss: 1.8137 - accuracy: 0.5048 - val_loss: 1.6122 - val_accuracy: 0.7523\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 201s 429ms/step - loss: 1.6561 - accuracy: 0.5488 - val_loss: 1.4136 - val_accuracy: 0.7745\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 201s 429ms/step - loss: 1.4983 - accuracy: 0.5893 - val_loss: 1.2251 - val_accuracy: 0.7877\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 214s 457ms/step - loss: 1.3604 - accuracy: 0.6152 - val_loss: 1.0640 - val_accuracy: 0.8009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f3e76a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_data(dataset)\n",
    "model = get_model(dataset)\n",
    "\n",
    "model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adadelta',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_test, Y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/model_%s.h5' % dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craft adversarial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_PARAMS = {\n",
    "    'mnist': {'eps': 0.300, 'eps_iter': 0.010},\n",
    "    'cifar': {'eps': 0.050, 'eps_iter': 0.005},\n",
    "    'svhn': {'eps': 0.130, 'eps_iter': 0.010}\n",
    "}\n",
    "attack = 'all'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(x, predictions, eps, clip_min=None, clip_max=None, y=None):\n",
    "    \"\"\"\n",
    "    Computes symbolic TF tensor for the adversarial samples. This must\n",
    "    be evaluated with a session.run call.\n",
    "    :param x: the input placeholder\n",
    "    :param predictions: the model's output tensor\n",
    "    :param eps: the epsilon (input variation parameter)\n",
    "    :param clip_min: optional parameter that can be used to set a minimum\n",
    "                    value for components of the example returned\n",
    "    :param clip_max: optional parameter that can be used to set a maximum\n",
    "                    value for components of the example returned\n",
    "    :param y: the output placeholder. Use None (the default) to avoid the\n",
    "            label leaking effect.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss\n",
    "    if y is None:\n",
    "        # In this case, use model predictions as ground truth\n",
    "        y = tf.to_float(\n",
    "            tf.equal(predictions,\n",
    "                     tf.reduce_max(predictions, 1, keep_dims=True)))\n",
    "    y = y / tf.reduce_sum(y, 1, keep_dims=True)\n",
    "    logits, = predictions.op.inputs\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    )\n",
    "\n",
    "    # Define gradient of loss wrt input\n",
    "    grad, = tf.gradients(loss, x)\n",
    "\n",
    "    # Take sign of gradient\n",
    "    signed_grad = tf.sign(grad)\n",
    "\n",
    "    # Multiply by constant epsilon\n",
    "    scaled_signed_grad = eps * signed_grad\n",
    "\n",
    "    # Add perturbation to original example to obtain adversarial example\n",
    "    adv_x = tf.stop_gradient(x + scaled_signed_grad)\n",
    "\n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) and (clip_max is not None):\n",
    "        adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)\n",
    "\n",
    "    return adv_x\n",
    "    \n",
    "def fast_gradient_sign_method(sess, model, X, Y, eps, clip_min=None,\n",
    "                              clip_max=None, batch_size=256):\n",
    "    # Define TF placeholders for the input and output\n",
    "    x = tf.placeholder(tf.float32, shape=(None,) + X.shape[1:])\n",
    "    y = tf.placeholder(tf.float32, shape=(None,) + Y.shape[1:])\n",
    "    adv_x = fgsm(\n",
    "        x, model(x), eps=eps,\n",
    "        clip_min=clip_min,\n",
    "        clip_max=clip_max, y=y\n",
    "    )\n",
    "    X_adv, = batch_eval(\n",
    "        sess, [x, y], [adv_x],\n",
    "        [X, Y], args={'batch_size': batch_size}\n",
    "    )\n",
    "\n",
    "    return X_adv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Iterative Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_iterative_method(sess, model, X, Y, eps, eps_iter, nb_iter=50,\n",
    "                           clip_min=None, clip_max=None, batch_size=256):\n",
    "    # Define TF placeholders for the input and output\n",
    "    x = tf.placeholder(tf.float32, shape=(None,)+X.shape[1:])\n",
    "    y = tf.placeholder(tf.float32, shape=(None,)+Y.shape[1:])\n",
    "    # results will hold the adversarial inputs at each iteration of BIM;\n",
    "    # thus it will have shape (nb_iter, n_samples, n_rows, n_cols, n_channels)\n",
    "    results = np.zeros((nb_iter, X.shape[0],) + X.shape[1:])\n",
    "    # Initialize adversarial samples as the original samples, set upper and\n",
    "    # lower bounds\n",
    "    X_adv = X\n",
    "    X_min = X_adv - eps\n",
    "    X_max = X_adv + eps\n",
    "    print('Running BIM iterations...')\n",
    "    # \"its\" is a dictionary that keeps track of the iteration at which each\n",
    "    # sample becomes misclassified. The default value will be (nb_iter-1), the\n",
    "    # very last iteration.\n",
    "    def f(val):\n",
    "        return lambda: val\n",
    "    its = defaultdict(f(nb_iter-1))\n",
    "    # Out keeps track of which samples have already been misclassified\n",
    "    out = set()\n",
    "    for i in tqdm(range(nb_iter)):\n",
    "        adv_x = fgsm(\n",
    "            x, model(x), eps=eps_iter,\n",
    "            clip_min=clip_min, clip_max=clip_max, y=y\n",
    "        )\n",
    "        X_adv, = batch_eval(\n",
    "            sess, [x, y], [adv_x],\n",
    "            [X_adv, Y], args={'batch_size': batch_size}\n",
    "        )\n",
    "        X_adv = np.maximum(np.minimum(X_adv, X_max), X_min)\n",
    "        results[i] = X_adv\n",
    "        # check misclassifieds\n",
    "        predictions = model.predict_classes(X_adv, batch_size=512, verbose=0)\n",
    "        misclassifieds = np.where(predictions != Y.argmax(axis=1))[0]\n",
    "        for elt in misclassifieds:\n",
    "            if elt not in out:\n",
    "                its[elt] = i\n",
    "                out.add(elt)\n",
    "\n",
    "    return its, results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian-based Saliency Map Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saliency_map_method(sess, model, X, Y, theta, gamma, clip_min=None,\n",
    "                        clip_max=None):\n",
    "    nb_classes = Y.shape[1]\n",
    "    X_adv = np.zeros_like(X)\n",
    "    # Instantiate a SaliencyMapMethod attack object\n",
    "    jsma = SaliencyMapMethod(model, back='tf', sess=sess)\n",
    "    jsma_params = {'theta': theta, 'gamma': gamma,\n",
    "                   'clip_min': clip_min, 'clip_max': clip_max,\n",
    "                   'y_target': None}\n",
    "    for i in tqdm(range(len(X))):\n",
    "        # Get the sample\n",
    "        sample = X[i:(i+1)]\n",
    "        # First, record the current class of the sample\n",
    "        current_class = int(np.argmax(Y[i]))\n",
    "        # Randomly choose a target class\n",
    "        target_class = np.random.choice(other_classes(nb_classes,\n",
    "                                                      current_class))\n",
    "        # This call runs the Jacobian-based saliency map approach\n",
    "        one_hot_target = np.zeros((1, nb_classes), dtype=np.float32)\n",
    "        one_hot_target[0, target_class] = 1\n",
    "        jsma_params['y_target'] = one_hot_target\n",
    "        X_adv[i] = jsma.generate_np(sample, **jsma_params)\n",
    "\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect.attacks import (fast_gradient_sign_method, basic_iterative_method,\n",
    "                            saliency_map_method)\n",
    "\n",
    "def craft_one_type(sess, model, X, Y, dataset, attack, batch_size):\n",
    "    if attack == 'fgsm':\n",
    "        # FGSM attack\n",
    "        print('Crafting fgsm adversarial samples...')\n",
    "        X_adv = fast_gradient_sign_method(\n",
    "            sess, model, X, Y, eps=ATTACK_PARAMS[dataset]['eps'], clip_min=0.,\n",
    "            clip_max=1., batch_size=batch_size\n",
    "        )\n",
    "    elif attack in ['bim-a', 'bim-b']:\n",
    "        # BIM attack\n",
    "        print('Crafting %s adversarial samples...' % attack)\n",
    "        its, results = basic_iterative_method(\n",
    "            sess, model, X, Y, eps=ATTACK_PARAMS[dataset]['eps'],\n",
    "            eps_iter=ATTACK_PARAMS[dataset]['eps_iter'], clip_min=0.,\n",
    "            clip_max=1., batch_size=batch_size\n",
    "        )\n",
    "        if attack == 'bim-a':\n",
    "            # BIM-A\n",
    "            # For each sample, select the time step where that sample first\n",
    "            # became misclassified\n",
    "            X_adv = np.asarray([results[its[i], i] for i in range(len(Y))])\n",
    "        else:\n",
    "            # BIM-B\n",
    "            # For each sample, select the very last time step\n",
    "            X_adv = results[-1]\n",
    "    elif attack == 'jsma':\n",
    "        # JSMA attack\n",
    "        print('Crafting jsma adversarial samples. This may take a while...')\n",
    "        X_adv = saliency_map_method(\n",
    "            sess, model, X, Y, theta=1, gamma=0.1, clip_min=0., clip_max=1.\n",
    "        )\n",
    "    else:\n",
    "        # TODO: CW attack\n",
    "        raise NotImplementedError('CW attack not yet implemented.')\n",
    "    _, acc = model.evaluate(X_adv, Y, batch_size=batch_size,\n",
    "                            verbose=0)\n",
    "    print(\"Model accuracy on the adversarial test set: %0.2f%%\" % (100 * acc))\n",
    "    np.save('../data/Adv_%s_%s.npy' % (dataset, attack), X_adv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_learning_phase(0)\n",
    "model = load_model('../data/model_%s.h5' % dataset)\n",
    "\n",
    "_, _, X_test, Y_test = get_data(dataset)\n",
    "_, acc = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "print(\"Accuracy on the test set: %0.2f%%\" % (100*acc))\n",
    "    \n",
    "if attack == 'all':\n",
    "    for attack in ['fgsm', 'bim-a', 'bim-b', 'jsma', 'cw']:\n",
    "        craft_one_type(sess, model, X_test, Y_test, dataset, attack, batch_size)\n",
    "else:\n",
    "    craft_one_type(sess, model, X_test, Y_test, dataset, attack, batch_size)\n",
    "    print('Adversarial samples crafted and saved to data/ subfolder.')\n",
    "    sess.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect adv samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect.util import (get_data, get_noisy_samples, get_mc_predictions,\n",
    "                         get_deep_representations, score_samples, normalize,\n",
    "                         train_lr, compute_roc)\n",
    "import warnings\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Optimal KDE bandwidths that were determined from CV tuning\n",
    "BANDWIDTHS = {'mnist': 1.20, 'cifar': 0.26, 'svhn': 1.00}\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    assert args.dataset in ['mnist', 'cifar', 'svhn'], \\\n",
    "        \"Dataset parameter must be either 'mnist', 'cifar' or 'svhn'\"\n",
    "    assert args.attack in ['fgsm', 'bim-a', 'bim-b', 'jsma', 'cw', 'all'], \\\n",
    "        \"Attack parameter must be either 'fgsm', 'bim-a', 'bim-b', \" \\\n",
    "        \"'jsma' or 'cw'\"\n",
    "    assert os.path.isfile('../data/model_%s.h5' % args.dataset), \\\n",
    "        'model file not found... must first train model using train_model.py.'\n",
    "    assert os.path.isfile('../data/Adv_%s_%s.npy' %\n",
    "                          (args.dataset, args.attack)), \\\n",
    "        'adversarial sample file not found... must first craft adversarial ' \\\n",
    "        'samples using craft_adv_samples.py'\n",
    "    print('Loading the data and model...')\n",
    "    # Load the model\n",
    "    model = load_model('../data/model_%s.h5' % args.dataset)\n",
    "    # Load the dataset\n",
    "    X_train, Y_train, X_test, Y_test = get_data(args.dataset)\n",
    "    # Check attack type, select adversarial and noisy samples accordingly\n",
    "    print('Loading noisy and adversarial samples...')\n",
    "    if args.attack == 'all':\n",
    "        # TODO: implement 'all' option\n",
    "        #X_test_adv = ...\n",
    "        #X_test_noisy = ...\n",
    "        raise NotImplementedError(\"'All' types detector not yet implemented.\")\n",
    "    else:\n",
    "        # Load adversarial samples\n",
    "        X_test_adv = np.load('../data/Adv_%s_%s.npy' % (args.dataset,\n",
    "                                                        args.attack))\n",
    "        # Craft an equal number of noisy samples\n",
    "        X_test_noisy = get_noisy_samples(X_test, X_test_adv, args.dataset,\n",
    "                                         args.attack)\n",
    "    # Check model accuracies on each sample type\n",
    "    for s_type, dataset in zip(['normal', 'noisy', 'adversarial'],\n",
    "                               [X_test, X_test_noisy, X_test_adv]):\n",
    "        _, acc = model.evaluate(dataset, Y_test, batch_size=args.batch_size,\n",
    "                                verbose=0)\n",
    "        print(\"Model accuracy on the %s test set: %0.2f%%\" %\n",
    "              (s_type, 100 * acc))\n",
    "        # Compute and display average perturbation sizes\n",
    "        if not s_type == 'normal':\n",
    "            l2_diff = np.linalg.norm(\n",
    "                dataset.reshape((len(X_test), -1)) -\n",
    "                X_test.reshape((len(X_test), -1)),\n",
    "                axis=1\n",
    "            ).mean()\n",
    "            print(\"Average L-2 perturbation size of the %s test set: %0.2f\" %\n",
    "                  (s_type, l2_diff))\n",
    "    # Refine the normal, noisy and adversarial sets to only include samples for\n",
    "    # which the original version was correctly classified by the model\n",
    "    preds_test = model.predict_classes(X_test, verbose=0,\n",
    "                                       batch_size=args.batch_size)\n",
    "    inds_correct = np.where(preds_test == Y_test.argmax(axis=1))[0]\n",
    "    X_test = X_test[inds_correct]\n",
    "    X_test_noisy = X_test_noisy[inds_correct]\n",
    "    X_test_adv = X_test_adv[inds_correct]\n",
    "\n",
    "    ## Get Bayesian uncertainty scores\n",
    "    print('Getting Monte Carlo dropout variance predictions...')\n",
    "    uncerts_normal = get_mc_predictions(model, X_test,\n",
    "                                        batch_size=args.batch_size) \\\n",
    "        .var(axis=0).mean(axis=1)\n",
    "    uncerts_noisy = get_mc_predictions(model, X_test_noisy,\n",
    "                                       batch_size=args.batch_size) \\\n",
    "        .var(axis=0).mean(axis=1)\n",
    "    uncerts_adv = get_mc_predictions(model, X_test_adv,\n",
    "                                     batch_size=args.batch_size) \\\n",
    "        .var(axis=0).mean(axis=1)\n",
    "\n",
    "    ## Get KDE scores\n",
    "    # Get deep feature representations\n",
    "    print('Getting deep feature representations...')\n",
    "    X_train_features = get_deep_representations(model, X_train,\n",
    "                                                batch_size=args.batch_size)\n",
    "    X_test_normal_features = get_deep_representations(model, X_test,\n",
    "                                                      batch_size=args.batch_size)\n",
    "    X_test_noisy_features = get_deep_representations(model, X_test_noisy,\n",
    "                                                     batch_size=args.batch_size)\n",
    "    X_test_adv_features = get_deep_representations(model, X_test_adv,\n",
    "                                                   batch_size=args.batch_size)\n",
    "    # Train one KDE per class\n",
    "    print('Training KDEs...')\n",
    "    class_inds = {}\n",
    "    for i in range(Y_train.shape[1]):\n",
    "        class_inds[i] = np.where(Y_train.argmax(axis=1) == i)[0]\n",
    "    kdes = {}\n",
    "    warnings.warn(\"Using pre-set kernel bandwidths that were determined \"\n",
    "                  \"optimal for the specific CNN models of the paper. If you've \"\n",
    "                  \"changed your model, you'll need to re-optimize the \"\n",
    "                  \"bandwidth.\")\n",
    "    for i in range(Y_train.shape[1]):\n",
    "        kdes[i] = KernelDensity(kernel='gaussian',\n",
    "                                bandwidth=BANDWIDTHS[args.dataset]) \\\n",
    "            .fit(X_train_features[class_inds[i]])\n",
    "    # Get model predictions\n",
    "    print('Computing model predictions...')\n",
    "    preds_test_normal = model.predict_classes(X_test, verbose=0,\n",
    "                                              batch_size=args.batch_size)\n",
    "    preds_test_noisy = model.predict_classes(X_test_noisy, verbose=0,\n",
    "                                             batch_size=args.batch_size)\n",
    "    preds_test_adv = model.predict_classes(X_test_adv, verbose=0,\n",
    "                                           batch_size=args.batch_size)\n",
    "    # Get density estimates\n",
    "    print('computing densities...')\n",
    "    densities_normal = score_samples(\n",
    "        kdes,\n",
    "        X_test_normal_features,\n",
    "        preds_test_normal\n",
    "    )\n",
    "    densities_noisy = score_samples(\n",
    "        kdes,\n",
    "        X_test_noisy_features,\n",
    "        preds_test_noisy\n",
    "    )\n",
    "    densities_adv = score_samples(\n",
    "        kdes,\n",
    "        X_test_adv_features,\n",
    "        preds_test_adv\n",
    "    )\n",
    "\n",
    "    ## Z-score the uncertainty and density values\n",
    "    uncerts_normal_z, uncerts_adv_z, uncerts_noisy_z = normalize(\n",
    "        uncerts_normal,\n",
    "        uncerts_adv,\n",
    "        uncerts_noisy\n",
    "    )\n",
    "    densities_normal_z, densities_adv_z, densities_noisy_z = normalize(\n",
    "        densities_normal,\n",
    "        densities_adv,\n",
    "        densities_noisy\n",
    "    )\n",
    "\n",
    "    ## Build detector\n",
    "    values, labels, lr = train_lr(\n",
    "        densities_pos=densities_adv_z,\n",
    "        densities_neg=np.concatenate((densities_normal_z, densities_noisy_z)),\n",
    "        uncerts_pos=uncerts_adv_z,\n",
    "        uncerts_neg=np.concatenate((uncerts_normal_z, uncerts_noisy_z))\n",
    "    )\n",
    "\n",
    "    ## Evaluate detector\n",
    "    # Compute logistic regression model predictions\n",
    "    probs = lr.predict_proba(values)[:, 1]\n",
    "    # Compute AUC\n",
    "    n_samples = len(X_test)\n",
    "    # The first 2/3 of 'probs' is the negative class (normal and noisy samples),\n",
    "    # and the last 1/3 is the positive class (adversarial samples).\n",
    "    _, _, auc_score = compute_roc(\n",
    "        probs_neg=probs[:2 * n_samples],\n",
    "        probs_pos=probs[2 * n_samples:]\n",
    "    )\n",
    "    print('Detector ROC-AUC score: %0.4f' % auc_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e30f898148fcd774fe92149405a32a1d675cf5330dbd168e754c453d75350896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
